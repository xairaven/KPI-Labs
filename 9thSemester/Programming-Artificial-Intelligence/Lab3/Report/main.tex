\documentclass{report}

%%%%%%%%%%%%%%%%%%%%%%%%%% Загальна інформація %%%%%%%%%%%%%%%%%%%%%%%%%%
\Subject {Системи штучного інтелекту}
\LabTitle{Логістична регресія}
\LabReport{Практична робота \#3}


%----------------------------------------------------
%	  Персональна інформація виконавця завдання
%----------------------------------------------------
\Done{Виконав:}  % Поставте правильне закінчення


\Surname{Ковальов}  % Вкажіть своє прізвище
\Name{Олександр}			% та ім'я
\Group {ІМ-51мн}     % група в якій навчаєтесь
\YearOfStudying {1} % курс навчання


%%%%%%%%%%%%%%%%%%%%%%%%%%	 ПОЧАТОК ЗВІТУ	 %%%%%%%%%%%%%%%%%%%%%%%%
\startDocument


\section{Логістична регресія}

\subsection{Реалізація логістичної регресії}

\subsubsection{Ініціалізація параметрів (\texttt{parameters\_inititalization})}
	
Функція ініціалізує ваги \texttt{W} невеликими випадковими значеннями та зсув \texttt{b} нулем.

\begin{lstlisting}[language=Python, style=mypython, frame=none]
def parameters_initialization(m):
	W = np.random.randn(1, m) * 0.01
	b = 0.0
	return W, b
\end{lstlisting}

\subsubsection{Пряме поширення (\texttt{forwardPropagate})}

Функція \texttt{forwardPropagate} обчислює лінійну комбінацію входів та ваг (\texttt{z}) і застосовує до неї сигмоїдну функцію активації для отримання прогнозу (\texttt{y\_hat}).

\begin{lstlisting}[language=Python, style=mypython, frame=none]
	def forwardPropagate(X, W, b):
		z = np.dot(W, X.T) + b
		y_hat = 1 / (1 + np.exp(-z))
		return z, y_hat
\end{lstlisting}

\subsubsection{Функція втрат (\texttt{cost})}
Розраховує усереднену бінарну перехресну ентропію, яка вимірює розбіжність між прогнозованими значеннями та істинними мітками.

\begin{lstlisting}[language=Python, style=mypython, frame=none]
	def cost(n, y_hat, y_true):
		ep = 10E-10
		y_hat_flat = y_hat.reshape(-1)
		J = - (1.0 / n) * np.sum(y_true * np.log(y_hat_flat + ep) + (1 - y_true) * np.log(1 - y_hat_flat + ep))
		return J
\end{lstlisting}

\subsubsection{Зворотне поширення (Обчислення градієнтів, \texttt{backwardPropagate})}

Обчислює градієнти (похідні) цільової функції відносно ваг та зсуву, які вказують напрямок для оновлення параметрів.

\begin{lstlisting}[language=Python, style=mypython, frame=none]
	def backwardPropagate(n, X, y_hat, y_true):
		y_hat_row = y_hat.reshape(1, -1)
		y_true_row = y_true.reshape(1, -1)
		dz = y_hat_row - y_true_row
		dW = (1.0 / n) * np.dot(dz, X)
		db = (1.0 / n) * np.sum(dz)
		return dW, db
\end{lstlisting}

\subsubsection{Оновлення параметрів (\texttt{update})}

Виконує крок градієнтного спуску, оновлюючи ваги \texttt{W} та зсув \texttt{b} для мінімізації цільової функції.

\begin{lstlisting}[language=Python, style=mypython, frame=none]
	def update(lr, dW, db, W, b):
		W = W - lr * dW
		b = b - lr * db
		return W, b
\end{lstlisting}

\subsection{Результати експериментів}

Модель була навчена з такими гіперпараметрами:

\begin{enumerate}
	\item Швидкість навчання (\texttt{$\alpha$}): 0.001
	\item Кількість ітерацій (\texttt{n\_iters}): 1000
\end{enumerate}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{ |>{\centering}p{5cm}|c|>{\centering}p{2.2cm}| }
			\hline
			\rowcolor{lightgray} \multicolumn{2}{|c|}{Показник} & Значення \tabularnewline
			\hline
			\multicolumn{2}{|c|}{Точність на тестовій вибірці} & 0.8859 (88.59\%) \tabularnewline
			\hline
			\multicolumn{2}{|c|}{Фінальна втрата на тестовій вибірці} & 1.6788 \tabularnewline
			\hline
			\multicolumn{2}{|c|}{Фінальна втрата на навчальній вибірці} &  1.6735 (на 980-й ітерації)  \tabularnewline
			\hline
		\end{tabular}
	\end{center}
\end{table}

Графік зміни втрат під час навчання показує, що цільова функція зменшується, але дуже шумно та нестабільно. 

\begin{figure}[!htb]
	\center
	\includegraphics[height=12cm]{01}
\end{figure}

Це, ймовірно, пов'язано з тим, що ознаки в наборі даних не були масштабовані і мають дуже різні діапазони значень.

Дослідження впливу гіперпараметрів показує, що швидкість навчання (\texttt{lr}) є критично важливим параметром. При низькому значенні, наприклад \texttt{lr = 0.00001}, модель навчається надто повільно, і за 1000 ітерацій цільова функція не встигає досягти мінімуму. Це призводить до недонавчання (underfitting) та низької точності. З іншого боку, занадто висока швидкість навчання, як \texttt{lr = 0.01}, змушує алгоритм "перестрибувати" мінімум, через що втрати хаотично коливаються або зростають, роблячи навчання неможливим.

Аналогічно, кількість ітерацій (\texttt{n\_iters}) має бути достатньою для збіжності моделі. Якщо зупинити навчання завчасно (напр., при 200 ітераціях), модель залишиться недонавченою, оскільки не встигне оптимально налаштувати свої ваги. І навпаки, значне збільшення кількості ітерацій, скажімо до 10000, може дещо покращити точність, але після досягнення плато це лише збільшує час навчання без суттєвого результату.

\subsection{Допомога}
Робота була виконана самостійно.


\subsection{Висновки}
Проведена робота демонструє повний цикл реалізації логістичної регресії з нуля. Аналіз гіперпараметрів дозволяє зробити висновок, що їх вибір -- це завжди компроміс. Крім того, висока нестабільність цільової функції під час навчання вказує на необхідність попередньої обробки даних, зокрема масштабування ознак. Стандартизація або нормалізація даних, найімовірніше, зробила б процес збіжності більш плавним і стабільним, потенційно підвищивши підсумкову точність. Таким чином, успішне навчання моделі вимагає як ретельного підбору гіперпараметрів, так і якісної підготовки даних. 

\end{document}
